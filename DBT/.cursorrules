Rules for cursor

Command Execution Rule (Critical)
Cursor must execute shell commands one per line, never combined with &&, never chained, and never combined with quoted paths. 
If you write && you die immediately. Dont do it.

The sequence for running DBT is:
1. Change directory to DBT folder: `cd DBT`
2. Load environment variables (MUST be done as THREE separate commands):
   - `set -a`
   - `source ../.env`
   - `set +a`
3. Export TARGET_SCHEMA from migration_config.yml:
   - Read target_schema from migration_config.yml in parent directory
   - `export TARGET_SCHEMA="<value_from_config>"`
4. Run DBT using the virtual environment:
   - `./redshift_env/bin/dbt run --select clients.<client_name>.<model_name> --profiles-dir . --project-dir .`

CRITICAL: The .env file MUST be loaded before running DBT. The profiles.yml file requires REDSHIFT_HOST, REDSHIFT_USER, REDSHIFT_PASSWORD, REDSHIFT_DB, and TARGET_SCHEMA environment variables. If these are not loaded, DBT will fail with "REDSHIFT_HOST not provided" errors.

Each step is its own terminal command. Never combine them with &&.

This rule completely eliminates the broken-eval behaviour you keep seeing.

1. DBT + MIGRATION CONTEXT
- DBT models live under models/.
- Client-specific models live under models/clients/<client_name_folder>.
- CRM template models live under models/<crm_template_folder>.
- Table knowledge base lives under knowledge_base/table/<table_name_folder>.
- The parent directory contains:
  - an .env file with Redshift credentials,
  - a migration_config.yml with client_name, source_crm, source_schema, target_schema, master_id.
- Cursor must never invent schema fields or warehouse assumptions.
- Cursor must ask the user whenever any structure, field, or requirement is unclear and cannot be inferred from config or code.

2. DBT KNOWLEDGE BASE (CRM + TABLE)
- knowledge_base/crm/ holds general rules for how each CRM behaves (naming patterns, null patterns, joins, location structure, source tables, utility models, quirks, etc.).
- knowledge_base/table/ holds per-table logic on how you should solve the transformation, fallback rules, field nuances, column expectations, known issues.
- Cursor must always consult crm/ for CRM-wide behaviour BEFORE running models or validations.
- Cursor must always consult table/ for table-level rules.
- Cursor must combine crm/ + table/ knowledge when reasoning about any transformation or QA failure.
- **CRITICAL**: If patterns are discovered during migration that differ from or are missing from the CRM knowledge base, Cursor MUST update the knowledge base file to document these patterns for future migrations.
- **CRITICAL STEP 2A: Check Knowledge Base BEFORE Running DBT Model**
  - BEFORE running any DBT model, Cursor MUST:
    - Extract the table name from the model filename (e.g., 3_people_bh → "people", 12_candidates_bh → "candidates")
    - Check if a knowledge base file exists: DBT/knowledge_base/table/<table_name>.yaml
    - If the knowledge base file exists, Cursor MUST read it COMPLETELY, especially looking for sections titled "## CRITICAL:" or "CRITICAL:"
    - **IF the knowledge base contains a "CRITICAL: [Something] Mapping Required Before Processing" section:**
      - Cursor MUST STOP execution immediately
      - Cursor MUST prompt the user using the EXACT prompt text provided in the knowledge base file
      - Cursor MUST wait for the user's explicit response before proceeding
      - Cursor MUST NOT run the DBT model until the user has confirmed the mapping strategy
      - This is a BLOCKING step - do not proceed to DBT execution until mapping is confirmed
    - Common tables requiring mapping prompts:
      - **companies**: relationship/status mapping (maps source status to 'none', 'target', or 'client')
      - **candidates**: status mapping (maps source candidate_status to target status values)
      - **projects**: status/close_reason mapping (maps job_status to state and close_reason)
      - **person_notes**: user mapping (maps source user_id to atlas user IDs)
      - **company_notes**: user mapping (maps source user_id to atlas user IDs)
  - **This check MUST happen BEFORE running DBT - it is a mandatory blocking step**
- **CRITICAL STEP 2B: Row Count Validation (MUST happen IMMEDIATELY after DBT, before knowledge base validation)**
  - **CRITICAL**: This check MUST happen immediately after each DBT model run completes successfully, BEFORE knowledge base validation.
  - **APPLIES TO ALL MODELS**: This validation applies to ALL models including utility models (people_to_import, companies_to_import, etc.)
  - **NEVER skip this step** - it catches critical issues like 0-record outputs early.
  - **For utility models**: If a utility model has 0 rows when source data exists, this will cause downstream models to fail - it's still a CRITICAL error that must be fixed.
  - After DBT model completes, Cursor MUST:
    - Extract the table name from the model filename (e.g., 13_meetings_bh → "meetings_bh")
    - Run the row count validation script: `python ../client_investigation/<client_name>/check_row_count.py <model_name> <table_name> [source_table]`
    - The script will:
      - Check if target table has 0 rows
      - **VERIFY SOURCE DATA**: If target has 0 rows, the script MUST check the source table to determine if this is acceptable
      - **CRITICAL DISTINCTION**:
        - If target has 0 rows AND source has data → CRITICAL ERROR (transformation bug - must fix immediately)
        - If target has 0 rows AND source also has 0 rows → ACCEPTABLE (proceed normally)
        - If target has 0 rows but source cannot be verified → CRITICAL ERROR (manual investigation required)
      - Compare source vs target row counts (warn if >50% difference, info if >10%)
      - Exit with error code if 0 rows detected when source has data
    - **IF 0 rows detected and source has data:**
      - This is a CRITICAL transformation bug (likely WHERE clause, JOIN, or filter issue)
      - **CRITICAL RULE**: Cursor MUST NOT document this as "expected behavior" or "acceptable" - it MUST be fixed
      - **Common causes to investigate**:
        - INNER JOIN conditions that are too restrictive (should be LEFT JOIN)
        - WHERE clause filters that exclude all valid data
        - Empty agency filters or other configuration that incorrectly evaluates to exclude all rows
        - Data type mismatches in JOIN conditions
        - Incorrect join logic (e.g., joining on wrong columns, using UUIDs where numeric IDs are needed)
        - Model trying to join intermediate tables when it should source directly from source tables
      - Cursor MUST:
        1. Investigate the DBT model logic thoroughly
        2. Check if the model should source directly from source tables instead of joining intermediate tables
        3. Verify join conditions match the actual data relationships
        4. Check template model to see if it has the same issue or uses a different approach
        5. Fix the DBT model immediately
        6. Rerun DBT for that model
        7. Rerun row count validation
        8. Do NOT proceed to knowledge base validation until row count is > 0 OR source is verified to have 0 rows
      - **REMEMBER**: If source has data, target MUST have data (or a very good reason why it doesn't - which should be rare and explicitly documented)
    - **IF 0 rows detected and source also has 0 rows (verified):**
      - This is ACCEPTABLE - proceed to knowledge base validation
    - **IF row count is acceptable (> 0):**
      - Proceed to Step 2C (Utility Model Validation), then Step 2D (Optional Manual Migration Comparison) if applicable, then Step 2E (Knowledge Base Validation)
    - This step prevents situations where models silently produce 0 rows due to filter/join issues, while allowing legitimate cases where source data doesn't exist
    - **REMEMBER**: If source has data, target MUST have data (or a very good reason why it doesn't - which should be rare and explicitly documented)
    - **REMEMBER**: If source has data, target MUST have data (or a very good reason why it doesn't - which should be rare and explicitly documented)

- **CRITICAL STEP 2C: Utility Model Validation (MUST happen after row count validation for key tables)**
  - **CRITICAL**: This check MUST happen after row count validation for key tables (people, companies, users) to ensure utility models are not too restrictive.
  - **PURPOSE**: Prevent utility models from filtering out valid records that should be included based on source data patterns.
  - **PRINCIPLE**: Utility models should include ALL records (after basic filters), not just those with specific relationships. The goal is to migrate all valid data, not just data with relationships.
  - **CRITICAL**: Cursor MUST read the CRM knowledge base (`knowledge_base/crm/<source_crm>.yml`) BEFORE running this validation to understand CRM-specific patterns.
  - After row count validation passes for key tables (people, companies, users), Cursor MUST:
    - **Check CRM knowledge base**: Read `knowledge_base/crm/<source_crm>.yml` for CRM-specific utility model requirements
    - **If CRM knowledge base doesn't exist or is incomplete**: Document the patterns discovered during migration and update the knowledge base file
    - **For each utility model** (e.g., `people_to_import`, `companies_to_import`):
      - Check if CRM knowledge base defines expected patterns for this utility model
      - If defined, validate utility model against source data using CRM-specific patterns from knowledge base
      - If not defined, validate generically: utility model should include all records from source table (after basic filters)
      - **CRITICAL**: If utility model has significantly fewer records (>10% difference from source after basic filters), it's too restrictive
      - **Expected pattern**: Utility models should include ALL records (after basic filters), matching the source count
      - **Common mistake**: Filtering to only records with specific relationships (comments, clients, jobs) - this excludes valid records
    - **IF utility model is too restrictive:**
      - This is a CRITICAL issue - utility models are filtering out valid records
      - Cursor MUST:
        1. Check CRM knowledge base for expected patterns
        2. Investigate the utility model logic
        3. Update utility models to include ALL records (after basic filters), not just those with relationships
        4. **Update CRM knowledge base**: If patterns discovered differ from knowledge base, update `knowledge_base/crm/<source_crm>.yml` to document the correct patterns
        5. Re-run utility models
        6. Re-run dependent models
        7. Re-verify row counts match source data patterns
    - **IF utility models match source data patterns:**
      - Proceed to Step 2D (Knowledge Base Validation)


- **CRITICAL STEP 2E: Knowledge Base Validation (MUST happen AFTER utility model validation, before QA)**
  - After row count validation passes, Cursor MUST check if a knowledge base file exists for that table (DBT/knowledge_base/table/<table_name>.yaml).
  - If a knowledge base file exists, Cursor MUST:
    - **Create investigation script** (if one doesn't exist): Create a Python validation script in `../client_investigation/<client_name>/validate_<table_name>.py` that automates the knowledge base checks
    - **Run the investigation script** to verify all points in "Key Issues to Check" and "What the Agent Should Check" sections are addressed
  - This validation must happen AFTER row count validation passes but BEFORE running QA Suite.
- **IMPORTANT**: When verifying issues, Cursor MUST compare source data vs transformed data to distinguish:
  - Source data quality issues (malformed values in source) → Document but DO NOT FIX
  - Transformation bugs (transformation corrupted valid source data) → MUST FIX
- **Delimiter-Separated Values**: If source has multiple values in one field (comma/slash/semicolon separated), transformation MUST split them - this is a requirement, not optional
- **CRITICAL WORKFLOW**: If transformation bugs are found (especially CRITICAL issues like NULL atlas_*_id columns):
  - **STEP 1**: Fix the transformation model (edit the client-specific SQL model file)
  - **STEP 2**: Rerun DBT for that model (using the same selection pattern: clients.<client_name>.<model_name>)
  - **STEP 3**: Re-run knowledge base validation to verify the fix
  - **STEP 4**: Only if knowledge base validation passes (or issues are documented as acceptable source quality), proceed to QA
  - **STEP 5**: Run QA Suite for the table prefix
  - **STEP 6**: If QA fails, repeat the fix → rerun DBT → re-check knowledge base → rerun QA loop until both knowledge base validation and QA pass
- Cursor must fix ONLY transformation bugs found in knowledge base checks before proceeding to QA.
- Source data quality issues should be documented as acceptable/expected and not fixed.
- **CRITICAL ISSUES** (like NULL atlas_*_id columns) MUST be fixed immediately - do not proceed to QA until these are resolved.

3. DATA AND MODEL INVESTIGATION RULES
- migration_config.yml is the single source of truth for client_name, source_crm, source_schema, target_schema, master_id.
- .env is the source of truth for Redshift connection variables.
- Cursor must use these values to determine:
  - which schema holds raw data (source_schema),
  - which schema holds transformed tables (target_schema),
  - which tables each DBT model is expected to materialize.
- Cursor must identify the target table for each DBT model.
- If any required field, structure, grain, or relationship is unclear, Cursor must ask the user.
- Cursor may generate and execute SQL queries via the project's database integration when available, and should use query results to inform its reasoning.

4. REDSHIFT ACCESS
- Redshift credentials include host, port, database name, username, and password.
- Cursor should assume the runtime environment or wrapper script loads these into the process before DBT or QA execution.
- Cursor must not hardcode credentials into code, models, or committed files.

5. DBT MODEL WORKFLOW 
- Before running DBT, you MUST load environment variables from the parent .env file:
  - Change to DBT directory: `cd DBT`
  - Load env variables (three separate commands, no &&):
    - `set -a`
    - `source ../.env`
    - `set +a`
  - Export TARGET_SCHEMA from migration_config.yml in parent directory
  - Verify REDSHIFT_HOST is loaded: `echo $REDSHIFT_HOST` (should show the host, not empty)
- You must use the virtual environment's dbt executable: `./redshift_env/bin/dbt` (not system dbt)
- CRM templates live under models/<crm_template_folder>.
- Client-specific models live under models/clients/<client_name_folder>.
- Cursor should only edit client-specific models unless the user explicitly approves template-level changes.
- For any model, Cursor must:
  - identify the target table and schema using target_schema and dbt configuration,
  - consider how that table is used in downstream DBT models and QA checks,
  - apply crm/ and table/ rules when interpreting fields (e.g. location, status, stages, IDs),
  - ask the user if grain, keys, or joins remain ambiguous after code inspection.
- If DBT fails with "REDSHIFT_HOST not provided" or similar env var errors, it means .env was not loaded correctly. Re-check the three-step env loading process.

6. MODEL INVESTIGATION FLOW
- Identify the DBT model file and its fully qualified name within the client namespace.
- Determine the target table using target_schema and the models configuration
- Examine
  - joins,
  - filters,
  - aggregations,
  - key selection,
  - use of mapping tables,
  - documented quirks from crm/ and table/.
- Suggest SQL to inspect:
  - row counts,
  - null rates,
  - duplicates on key columns,
  - distributions of important fields,
  - coverage of fallback component fields (e.g. city, state, country for location).
- Ask clarifying questions such as:
  - should this field exist for this client?
  - should this be unique per entity?
  - is it acceptable for this field to be null?
  - is this mapping table correct for this CRM?
- Cursor must not change business behaviour where requirements are unclear.

7. WRAPPER SCRIPT VS DIRECT DBT EXECUTION
- A wrapper script exists in the parent directory to prepare environment variables and run DBT with the correct profile and project directory.
- Cursor should treat the wrapper script as the default way to run DBT when possible.
- If the wrapper is unavailable or inappropriate for a specific task, Cursor may run DBT directly using the project's virtual environment DBT executable rather than a system-wide dbt.
- When running DBT directly, Cursor MUST follow this exact sequence (each command on its own line):
  1. `cd DBT` (change to DBT directory)
  2. `set -a` (enable automatic export of variables)
  3. `source ../.env` (load variables from parent .env file)
  4. `set +a` (disable automatic export)
  5. Verify env loaded: `echo $REDSHIFT_HOST` (should not be empty)
  6. Export TARGET_SCHEMA: `export TARGET_SCHEMA="<value_from_migration_config.yml>"`
  7. Run DBT: `./redshift_env/bin/dbt run --select clients.<client_name>.<model_name> --profiles-dir . --project-dir .`
- CRITICAL: Never skip the env loading steps. If REDSHIFT_HOST is missing, DBT will fail. Always verify env vars are loaded before running DBT.
- Cursor should run DBT commands itself in the integrated terminal and not ask for permission, unless required values are missing or ambiguous.

8. QA SUITE (CONCEPTUAL RULES)
- The QA Suite validates table groups and full sweeps after DBT models have been run.
- The QA Suite expects specific tables and fields in target_schema to exist and be correctly populated.
- When a QA failure occurs, Cursor must:
  - trace the failure back to the responsible DBT model or model group,
  - identify which fields are missing, mis-typed, or incorrectly populated,
  - propose precise, minimal updates to the relevant model(s),
  - ask for confirmation when behaviour is business-specific or client-specific.
- Cursor should trigger QA runs itself as part of the per-model loop defined in the parent rules, and interpret the results.

9. MODIFICATION GUARDRAILS
- Cursor must only modify models/clients/<client_name_folder> unless explicitly instructed to change templates or shared models.
- Cursor must preserve the integrity of dbt_project.yml and profiles.yml (do not introduce breaking config changes without clear intent).
- Cursor must avoid broad, global changes that would affect other clients or CRMs unless the user confirms that scope is desired.
- If a proposed modification may impact multiple clients or CRM templates, Cursor must explicitly state this impact and ask whether it is acceptable.

10. DATA INVESTIGATION (REQUIRED SQL STEP)
- When the user asks about missing values, coverage, fallbacks, enrichment, or field quality (for example: locality, metro, city, state, country, email, phone, title, industry), Cursor must:
  - identify the correct source table(s) in source_schema,
  - generate SQL to inspect raw CRM fields and their coverage,
  - execute SQL via the available database integration when possible and use results in its reasoning,
  - show or summarise SQL and results before proposing model changes,
  - use patterns (null rates, distinct values, distributions, component combinations) to guide fallback and enrichment logic.
- Example triggers include:
  - boosting location coverage,
  - filling missing locality or metro fields,
  - deriving fallback city/state/country,
  - examining phone/email/title/industry completeness,
  - diagnosing CRM-specific field sparsity.

11. REQUIRED SQL INSPECTION RULE
- Cursor must inspect raw data via SQL before proposing non-trivial DBT changes that affect field coverage or fallback logic.
- Cursor must:
  - identify the correct source table in source_schema,
  - generate and, when possible, execute SQL to understand data patterns,
  - present or summarise SQL and key findings,
  - ask the user when CRM or client-specific nuances are not documented.
- Recommendations must be based on observed data patterns rather than assumptions.

12. AUTOMATION BEHAVIOUR (DBT FOLDER)
- Cursor should run DBT and QA commands itself in the integrated terminal when required by the workflow, without asking for approval.
- ALWAYS follow the exact env loading sequence before running DBT:
  1. `cd DBT`
  2. `set -a`
  3. `source ../.env`
  4. `set +a`
  5. Verify: `echo $REDSHIFT_HOST` (must not be empty)
  6. Export TARGET_SCHEMA from migration_config.yml
  7. Run DBT command
- NEVER skip the env loading steps. This is the #1 cause of "REDSHIFT_HOST not provided" errors.
- Cursor should only pause to ask the user when:
  - required information (paths, schema names, client_name, table names) is missing or ambiguous,
  - a change could impact multiple clients or CRMs and needs scope confirmation,
  - a business rule or field meaning is unclear from existing config, crm/, and table/ knowledge.
- Cursor should avoid complex multi-line or heavily chained shell constructs; it should execute commands in simple, sequential steps to reduce shell errors.
- If DBT fails with environment variable errors, immediately check if .env was loaded correctly using `echo $REDSHIFT_HOST`.

13. CLIENT INVESTIGATION SCRIPTS
- **CRITICAL**: When creating test Python scripts, data comparison scripts, or any investigation scripts:
  - MUST create them in: `client_investigation/<client_name>/` (parent directory, not DBT/)
  - Read `client_name` from migration_config.yml in parent directory
  - Create the client-specific folder if it doesn't exist: `client_investigation/<client_name>/`
  - All test scripts, comparison scripts, and investigation tools MUST be placed in this client-specific folder
  - NEVER create orphaned scripts in DBT/tests/ or root directories - they must be organized by client
  - Example: For client "lechley_associates", scripts go in `../client_investigation/lechley_associates/`
  - **Timestamped Files**: When scripts are run, they should create timestamped copies of themselves (e.g., `validate_person_identities_2025-11-28_17-45-35.py`) and timestamped output files (e.g., `validate_person_identities_2025-11-28_17-45-35.txt`) for historical record-keeping
  - This keeps the database clean and organizes investigation work by client
- Test scripts should follow naming conventions: test_<purpose>.py or <purpose>_test.py
- Always load environment variables from parent .env before running test scripts.
- Test scripts should be reusable and well-documented.



