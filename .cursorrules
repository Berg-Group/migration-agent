MIGRATION PLAYBOOK (PARENT LEVEL)

REALLY IMPORTANT
- Cursor must execute shell commands one per line, never combined with &&, never chained, and never combined with quoted paths. 
- If you write && you die immediately. Don't do it.

The sequence for running DBT is:
- Change directory
- Load env
- Export schema
- Run DBT


High-level goal:
Cursor is responsible for driving the migration workflow end-to-end for each client: setting up client models, running DBT models one by one, running the QA Suite after each model, and looping until everything passes. Cursor should run commands itself in the integrated terminal wherever possible, and only stop to ask the user when required information is missing.

1. Read migration_config.yml first
- Open migration_config.yml in the parent directory.
- Retrieve at least: client_name, source_crm, source_schema, target_schema, master_id, tables_to_transform.
- Treat migration_config.yml as the source of truth for client-specific configuration.

2. Create / confirm the client workspace
- Ensure the client has a folder inside DBT/models/clients corresponding to client_name.
- **CRITICAL**: Copy ALL template SQL models from the appropriate CRM template folder (DBT/models/Templates/<source_crm>/*.sql) into DBT/models/clients/<client_name>.
- **IMPORTANT**: The `tables_to_transform` field in migration_config.yml is for documentation/tracking purposes only - it does NOT filter which templates to copy. ALL template models must be copied regardless of what's listed in `tables_to_transform`.
- Also copy any utility models from DBT/models/Templates/<source_crm>/utils/ if they exist.
- All editing for this migration must occur in the client folder, not in the template folders.
- After copying, verify that the number of model files in the client folder matches the number in the template folder (excluding Archive/ and other non-model files).

3. Determine the model run order
- Navigate (conceptually or via terminal) to DBT/models/clients/<client_name>.
- List the client SQL model files and sort them in natural numeric order (e.g. 1_, 2_, 3_…).
- Treat this as the canonical sequence for running models for this client.

4. Per-model workflow (for each model in order)
**CRITICAL: Models MUST be processed ONE AT A TIME, not in batches. Each model must complete its full workflow before moving to the next.**

**IMPORTANT**: This workflow applies to ALL models, including:
- Main transformation models (e.g., 0_users_bh.sql, 1_people_bh.sql, 3_companies_bh.sql)
- Utility models in utils/ subfolder (e.g., people_to_import.sql, companies_to_import.sql, people_dupes_bh.sql)
- All other models in the client folder

For each model file in DBT/models/clients/<client_name> (including utils/ subfolder, processed individually, not in groups):

**CRITICAL STEP 4A: Check Knowledge Base for Mapping Requirements (MUST DO BEFORE RUNNING DBT)**
- Extract the table name from the model filename (e.g., 3_people_bh → "people", 12_candidates_bh → "candidates", 10_projects_bh → "projects")
- Check if a knowledge base file exists: DBT/knowledge_base/table/<table_name>.yaml
- If the knowledge base file exists, Cursor MUST:
  - Read the ENTIRE knowledge base file, especially looking for sections titled "CRITICAL:" or "## CRITICAL:"
  - **IF the knowledge base contains a "CRITICAL: [Something] Mapping Required Before Processing" section:**
    - Cursor MUST STOP and prompt the user using the EXACT prompt text provided in the knowledge base file
    - Cursor MUST wait for the user's response before proceeding
    - Cursor MUST NOT run the DBT model until the user has confirmed the mapping strategy
    - This applies to mappings for: status, relationship, close_reason, user mapping, or any other field mappings
  - Common tables requiring mapping prompts:
    - **companies**: relationship/status mapping
    - **candidates**: status mapping
    - **projects**: status/close_reason mapping
    - **person_notes**: user mapping
    - **company_notes**: user mapping
- **This check MUST happen BEFORE running DBT - it is a blocking step**
- If no knowledge base file exists or no mapping requirements are found, proceed to Step 4B

**STEP 4B1: Configuration Validation (CRITICAL - MUST happen BEFORE running DBT models)**
- **CRITICAL**: Before running any DBT models, Cursor MUST run configuration validation to catch configuration issues that could cause data loss.
- Run: `python client_investigation/<client_name>/validate_config_and_filtering.py`
- This validation checks:
  - Domain variable is set (if required for CRM)
  - Users count is reasonable (not including all contacts)
  - People exclusion rate is reasonable (not over-filtering)
  - Users are not incorrectly included in people
- **IF validation finds CRITICAL issues:**
  - Cursor MUST STOP and fix the issues before proceeding
  - Do NOT run DBT models until configuration is correct
- **IF validation finds warnings:**
  - Cursor should investigate and verify if warnings are acceptable
  - Document findings before proceeding

**STEP 4B: Run DBT Model (ONE MODEL AT A TIME)**
- **CRITICAL**: Run DBT for ONLY ONE model at a time using: `--select clients.<client_name>.<model_name>`
- **NEVER run multiple models in a single DBT command** (e.g., do NOT use `--select clients.<client_name>.<model1> clients.<client_name>.<model2> ...`)
- Move execution context to the DBT directory.
- Ensure environment variables from the parent .env are loaded (using the project's standard method).
- Ensure target_schema is set according to migration_config.yml.
- Execute DBT for this single model only, using the clients.<client_name>.<model_name> selection pattern.
- Cursor should run these steps itself in the integrated terminal, without asking for approval, as long as all required information (paths, client_name, target_schema) is known.

**STEP 4C: Row Count Validation (CRITICAL - MUST happen IMMEDIATELY after DBT, before knowledge base validation)**
- **CRITICAL**: This check MUST happen immediately after each DBT model run completes successfully, BEFORE knowledge base validation.
- **APPLIES TO ALL MODELS**: This validation applies to ALL models including utility models (people_to_import, companies_to_import, etc.)
- **NEVER skip this step** - it catches critical issues like 0-record outputs early.
- **For utility models**: If a utility model has 0 rows when source data exists, this will cause downstream models to fail - it's still a CRITICAL error that must be fixed.
- After DBT model completes, Cursor MUST:
  - Extract the table name from the model filename (e.g., 13_meetings_bh → "meetings_bh")
  - Run the row count validation script: `python client_investigation/<client_name>/check_row_count.py <model_name> <table_name> [source_table]`
  - The script will:
    - Check if target table has 0 rows
    - **VERIFY SOURCE DATA**: If target has 0 rows, the script MUST check the source table to determine if this is acceptable
    - **CRITICAL DISTINCTION**:
      - If target has 0 rows AND source has data → CRITICAL ERROR (transformation bug - must fix immediately)
      - If target has 0 rows AND source also has 0 rows → ACCEPTABLE (proceed normally)
      - If target has 0 rows but source cannot be verified → CRITICAL ERROR (manual investigation required)
    - Compare source vs target row counts (warn if >50% difference, info if >10%)
    - Exit with error code if 0 rows detected when source has data
  - **IF 0 rows detected and source has data:**
    - This is a CRITICAL transformation bug (likely WHERE clause, JOIN, or filter issue)
    - **CRITICAL RULE**: Cursor MUST NOT document this as "expected behavior" or "acceptable" - it MUST be fixed
    - **Common causes to investigate**:
      - INNER JOIN conditions that are too restrictive (should be LEFT JOIN)
      - WHERE clause filters that exclude all valid data
      - Empty agency filters or other configuration that incorrectly evaluates to exclude all rows
      - Data type mismatches in JOIN conditions
      - Incorrect join logic (e.g., joining on wrong columns, using UUIDs where numeric IDs are needed)
    - Cursor MUST:
      1. Investigate the DBT model logic thoroughly
      2. Check if the model should source directly from source tables instead of joining intermediate tables
      3. Verify join conditions match the actual data relationships
      4. Fix the DBT model immediately
      5. Rerun DBT for that model
      6. Rerun row count validation
      7. Do NOT proceed to knowledge base validation until row count is > 0 OR source is verified to have 0 rows
  - **IF 0 rows detected and source also has 0 rows (verified):**
    - This is ACCEPTABLE - proceed to Step 5 (Knowledge Base Validation)
  - **IF row count is acceptable (> 0):**
    - Proceed to Step 4D (Utility Model Validation), then Step 5 (Knowledge Base Validation)
  - This step prevents situations where models silently produce 0 rows due to filter/join issues, while allowing legitimate cases where source data doesn't exist
  - **REMEMBER**: If source has data, target MUST have data (or a very good reason why it doesn't - which should be rare and explicitly documented)
  - **REMEMBER**: If source has data, target MUST have data (or a very good reason why it doesn't - which should be rare and explicitly documented)

**STEP 4D: Utility Model Validation (CRITICAL - MUST happen after row count validation for key tables)**
- **CRITICAL**: This check MUST happen after row count validation for key tables (people, companies, users) to ensure utility models are not too restrictive.
- **PURPOSE**: Prevent utility models from filtering out valid records that should be included based on source data patterns.
- **PRINCIPLE**: Utility models should include ALL records (after basic filters), not just those with specific relationships. The goal is to migrate all valid data, not just data with relationships.
- **CRITICAL**: Cursor MUST read the CRM knowledge base (`DBT/knowledge_base/crm/<source_crm>.yml`) BEFORE running this validation to understand CRM-specific patterns.
- After row count validation passes for key tables (people, companies, users), Cursor MUST:
  - **Check CRM knowledge base**: Read `DBT/knowledge_base/crm/<source_crm>.yml` for CRM-specific utility model requirements
  - **If CRM knowledge base doesn't exist or is incomplete**: Document the patterns discovered during migration and update the knowledge base file
  - **For each utility model** (e.g., `people_to_import`, `companies_to_import`):
    - Check if CRM knowledge base defines expected patterns for this utility model
    - If defined, validate utility model against source data using CRM-specific patterns from knowledge base
    - If not defined, validate generically: utility model should include all records from source table (after basic filters)
    - **CRITICAL**: If utility model has significantly fewer records (>10% difference from source after basic filters), it's too restrictive
    - **Expected pattern**: Utility models should include ALL records (after basic filters), matching the source count
    - **Common mistake**: Filtering to only records with specific relationships (comments, clients, jobs) - this excludes valid records
  - **IF utility model is too restrictive:**
    - This is a CRITICAL issue - utility models are filtering out valid records
    - Cursor MUST:
      1. Check CRM knowledge base for expected patterns
      2. Investigate the utility model logic
      3. Update utility models to include ALL records (after basic filters), not just those with relationships
      4. **Update CRM knowledge base**: If patterns discovered differ from knowledge base, update `DBT/knowledge_base/crm/<source_crm>.yml` to document the correct patterns
      5. Re-run utility models
      6. Re-run dependent models
      7. Re-verify row counts match source data patterns
  - **IF utility models match source data patterns:**
    - Proceed to Step 5 (Knowledge Base Validation)

5. Knowledge Base Validation (REQUIRED for all tables - MUST happen AFTER EACH model, not in batch)
- **CRITICAL**: This step MUST be completed for EACH model individually, immediately after that model's DBT run completes successfully.
- **NEVER skip this step or batch it with multiple models** - each model must have its knowledge base validation completed before moving to the next model.
- After each successful DBT model run, BEFORE running QA, Cursor MUST:
  - Extract the table name from the model filename (e.g., 3_people_rcrm → "people", 4_person_identities_rcrm → "person_identities")
  - Check if a knowledge base file exists: DBT/knowledge_base/table/<table_name>.yaml
  - If the knowledge base file exists, Cursor MUST:
    - Read and understand all points listed in the "Key Issues to Check" section
    - Read and understand all points in the "What the Agent Should Check" section
    - **Create investigation script** (if one doesn't exist): Create a Python validation script in `client_investigation/<client_name>/validate_<table_name>.py` that automates the knowledge base checks
    - **Run the investigation script** to execute SQL queries and data inspections to verify each point is addressed
    - The script should check for CRITICAL issues first (like NULL atlas_*_id columns), then other validation points
    - **CRITICAL DISTINCTION**: When verifying data quality issues, Cursor MUST compare source data vs transformed data to determine:
      - If source data has quality issues (missing dots, malformed values) → Document but DO NOT FIX (this is source data quality)
      - If transformation corrupted valid source data → MUST FIX (this is a transformation bug)
    - **Delimiter-Separated Values**: If source data has multiple values in a single field (e.g., `email1@example.com, email2@example.com`), transformation MUST split them into separate records - this is a transformation requirement, not optional
    - Document findings and fix ONLY transformation bugs before proceeding to QA
    - Do not attempt to fix source data quality issues - these should be documented as acceptable/expected
  - **CRITICAL WORKFLOW**: If transformation bugs are found (especially CRITICAL issues like NULL atlas_*_id columns):
    - **STEP 1**: Fix the transformation model (edit the client-specific SQL model file)
    - **STEP 2**: Rerun DBT for that model (using the same selection pattern: clients.<client_name>.<model_name>)
    - **STEP 3**: Re-run knowledge base validation to verify the fix
    - **STEP 4**: Only if knowledge base validation passes (or issues are documented as acceptable source quality), proceed to QA
    - **STEP 5**: Run QA Suite for the table prefix
    - **STEP 6**: If QA fails, repeat the fix → rerun DBT → re-check knowledge base → rerun QA loop until both knowledge base validation and QA pass
  - This step is MANDATORY and applies to ALL tables that have knowledge base files
  - Cursor must not skip this step even if QA would catch some issues - knowledge base checks are table-specific validations that may catch issues QA doesn't
  - **CRITICAL ISSUES** (like NULL atlas_*_id columns) MUST be fixed immediately - do not proceed to QA until these are resolved

6. Immediate QA after each model (ONE MODEL AT A TIME)
- **CRITICAL**: QA MUST be run for EACH model individually, immediately after knowledge base validation passes for that model.
- **NEVER run QA for all tables at once** (e.g., do NOT use `npm run qa` without a table prefix) until ALL individual models have passed their QA checks.
- After each successful model run and knowledge base validation, immediately run the QA Suite for the relevant table prefix.
- The table prefix is usually the part of the model filename before the CRM suffix (for example: 3_people_rcrm → "people").
- Use: `npm run qa <table_prefix>` (e.g., `npm run qa people`, `npm run qa projects`)
- Verify QA output before moving to the next model.
- Cursor should run the QA command(s) itself using the established QA Suite mechanism.
- Only after ALL individual models have passed QA should a full QA sweep be run (see Step 7).

7. Loop until clean (PER MODEL)
- **CRITICAL**: The fix → rerun → validate → QA loop applies to EACH model individually.
- **CRITICAL**: Cursor MUST STOP and fix issues before moving to the next model. This is NOT optional.
- **NEVER continue to the next model if the current model has:**
  - Row count validation failures (0 rows when source has data)
  - Knowledge base validation failures (CRITICAL issues like NULL atlas_*_id)
  - QA failures (any validation errors)
- If QA fails for a model: inspect the error, identify the responsible DBT model, adjust only the necessary logic in the client model, rerun the model, then rerun row count validation, then rerun knowledge base validation, then rerun the corresponding QA.
- Repeat this fix → rerun model → re-check row count → re-check knowledge base → rerun QA loop until QA passes for that model.
- **Proceed to the next model ONLY when the current model's QA is clean.**
- **NEVER move to the next model if the current one has QA failures or knowledge base validation issues.**
- **Common issues to check before proceeding:**
  - Empty agency filters causing 0 rows (check if `get_agency_filter()` returns `('')` and handle it)
  - INNER JOIN with empty users_bh causing 0 rows (use LEFT JOIN with COALESCE fallback)
  - Empty master_id causing NULL atlas_*_id (document or set master_id in config)
- When all client models have passed their individual QA checks, run the full QA Suite sweep once more (`npm run qa` without prefix) to confirm everything passes together.

8. Guardrails
- Do not modify template SQL files; only use them as a source to copy into client-specific folders.
- Ensure dbt_project.yml and profiles.yml remain aligned with migration_config.yml (source_schema, client_name, etc.).
- Always ensure environment variables are loaded from the parent .env before running DBT or QA.
- When DBT reports duplicate models, confirm that only client-specific models under clients.<client_name> are being selected.
- When fallback or default values are needed, use master_id and other values from migration_config.yml rather than introducing hardcoded constants.

9. Automation behaviour
- Cursor should run DBT models and QA checks automatically in the integrated terminal when the workflow requires it.
- Cursor should not ask for permission before running these commands.
- **CRITICAL EXCEPTION**: Cursor MUST pause and prompt the user when:
  - A knowledge base file contains a "CRITICAL: [Something] Mapping Required Before Processing" section (see Step 4A)
  - This is a BLOCKING step - DBT model execution MUST NOT proceed until user confirms mapping strategy
- Cursor must also pause and ask the user when:
  - a required value (path, client_name, schema name, table name) is missing or ambiguous,
  - a business rule is unclear (e.g. how a particular field should behave),
  - a proposed change could affect multiple clients or CRMs and scope needs confirmation.

10. Client Investigation Scripts
- **CRITICAL**: When creating test Python scripts, data comparison scripts, or any investigation scripts:
  - MUST create them in: `client_investigation/<client_name>/`
  - Read `client_name` from migration_config.yml
  - Create the client-specific folder if it doesn't exist: `client_investigation/<client_name>/`
  - All test scripts, comparison scripts, and investigation tools MUST be placed in this client-specific folder
  - NEVER create orphaned scripts in root directories or DBT/tests/ - they must be organized by client
  - Example: For client "lechley_associates", scripts go in `client_investigation/lechley_associates/`
  - **Timestamped Files**: When scripts are run, they should create timestamped copies of themselves (e.g., `validate_person_identities_2025-11-28_17-45-35.py`) and timestamped output files (e.g., `validate_person_identities_2025-11-28_17-45-35.txt`) for historical record-keeping
  - This keeps the database clean and organizes investigation work by client
